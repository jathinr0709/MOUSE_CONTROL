{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd4d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install numpy\n",
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b72d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyautogui for automating GUI interactions, such as moving the mouse, clicking, and keyboard actions\n",
    "!pip install pyautogui\n",
    "\n",
    "# Install MediaPipe, a library for machine learning pipelines, useful for tasks like hand and face tracking\n",
    "!pip install mediapipe\n",
    "\n",
    "# Install xvfb (X Virtual FrameBuffer), a display server that enables off-screen rendering, which is useful for headless environments\n",
    "!apt-get install -y xvfb\n",
    "\n",
    "# Install pyvirtualdisplay to create and manage virtual displays, allowing GUI applications to run in a headless environment\n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b03136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pynput, a library that allows control and monitoring of the mouse and keyboard.\n",
    "# Useful for creating automation scripts that simulate user input or listen for specific keyboard events.\n",
    "!pip install pynput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f89ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenCV for image processing and computer vision tasks\n",
    "import cv2\n",
    "\n",
    "# Import MediaPipe for machine learning-based pipelines, such as hand or pose tracking\n",
    "import mediapipe as mp\n",
    "\n",
    "# Import pyautogui for GUI automation, enabling control of the mouse and keyboard for screen interactions\n",
    "import pyautogui\n",
    "\n",
    "# Import random for generating random values, which could be used for various purposes, like randomizing movements or actions\n",
    "import random\n",
    "\n",
    "# Import Button and Controller from pynput.mouse to control the mouse programmatically, allowing clicks and movement\n",
    "from pynput.mouse import Button, Controller\n",
    "\n",
    "# Initialize a mouse controller instance, enabling programmatic mouse actions (e.g., moving, clicking)\n",
    "mouse = Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014c511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the angle between three points (a, b, c)\n",
    "def get_angle(a, b, c):\n",
    "    # Calculate the angle in radians between vectors ba and bc\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    # Convert the angle from radians to degrees and take the absolute value\n",
    "    angle = np.abs(np.degrees(radians))\n",
    "    return angle\n",
    "\n",
    "# Calculate the distance between two points in the landmark list\n",
    "def get_distance(landmark_ist):\n",
    "    # Ensure there are at least two landmarks to calculate the distance\n",
    "    if len(landmark_ist) < 2:\n",
    "        return\n",
    "    # Unpack the coordinates of the two points\n",
    "    (x1, y1), (x2, y2) = landmark_ist[0], landmark_ist[1]\n",
    "    # Compute the Euclidean distance between the points\n",
    "    L = np.hypot(x2 - x1, y2 - y1)\n",
    "    # Interpolate the distance to a scale of 0 to 1000\n",
    "    return np.interp(L, [0, 1], [0, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12040766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the screen width and height in pixels using pyautogui, which can be used for setting boundaries for mouse movements or GUI elements\n",
    "screen_width, screen_height = pyautogui.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the MediaPipe Hands module, which provides hand detection and tracking capabilities\n",
    "mpHands = mp.solutions.hands\n",
    "\n",
    "# Initialize the Hands object with custom parameters for hand detection and tracking\n",
    "hands = mpHands.Hands(\n",
    "    static_image_mode=False,           # False enables detection for live video streams\n",
    "    model_complexity=1,                # Sets model complexity; higher values may improve accuracy at a cost of speed\n",
    "    min_detection_confidence=0.7,      # Minimum confidence for the hand detection to be considered successful\n",
    "    min_tracking_confidence=0.7,       # Minimum confidence for tracking a detected hand between frames\n",
    "    max_num_hands=1                    # Sets the maximum number of hands to detect and track\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a74f6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the tip of the index finger from hand landmarks detected by MediaPipe\n",
    "def find_finger_tip(processed):\n",
    "    # Check if any hand landmarks were detected\n",
    "    if processed.multi_hand_landmarks:\n",
    "        # Access the landmarks of the first detected hand (assuming only one hand is detected)\n",
    "        hand_landmarks = processed.multi_hand_landmarks[0]\n",
    "        \n",
    "        # Get the landmark corresponding to the tip of the index finger\n",
    "        index_finger_tip = hand_landmarks.landmark[mpHands.HandLandmark.INDEX_FINGER_TIP]\n",
    "        \n",
    "        # Return the coordinates of the index finger tip (x, y, z)\n",
    "        return index_finger_tip\n",
    "    \n",
    "    # Return None if no hand is detected\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0901e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to move the mouse cursor to the position of the index finger tip\n",
    "def move_mouse(index_finger_tip):\n",
    "    # Check if the index finger tip coordinates are not None (i.e., a hand is detected)\n",
    "    if index_finger_tip is not None:\n",
    "        # Convert the normalized x and y coordinates (ranging from 0 to 1) to screen pixel values\n",
    "        x = int(index_finger_tip.x * screen_width)  # Multiply by screen width to get x-coordinate in pixels\n",
    "        y = int(index_finger_tip.y / 2 * screen_height)  # Divide by 2 to account for the flipped y-axis in screen coordinates\n",
    "        \n",
    "        # Move the mouse cursor to the calculated screen coordinates\n",
    "        pyautogui.moveTo(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e73c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect a left-click gesture based on hand landmarks and thumb-index finger distance\n",
    "def is_left_click(landmark_list, thumb_index_dist):\n",
    "    return (\n",
    "        # Check if the angle between the thumb and index finger is less than 50 degrees (indicating a closed hand)\n",
    "        get_angle(landmark_list[5], landmark_list[6], landmark_list[8]) < 50 and\n",
    "        \n",
    "        # Check if the angle between the middle finger and index finger is greater than 90 degrees (indicating an open hand)\n",
    "        get_angle(landmark_list[9], landmark_list[10], landmark_list[12]) > 90 and\n",
    "        \n",
    "        # Check if the distance between the thumb and index finger is greater than a threshold (indicating the fingers are apart enough for a click)\n",
    "        thumb_index_dist > 50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "321565cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect a right-click gesture based on hand landmarks and thumb-index finger distance\n",
    "def is_right_click(landmark_list, thumb_index_dist):\n",
    "    return (\n",
    "        # Check if the angle between the middle finger and the index finger is less than 50 degrees (indicating the fingers are close, forming a \"pinch\")\n",
    "        get_angle(landmark_list[9], landmark_list[10], landmark_list[12]) < 50 and\n",
    "        \n",
    "        # Check if the angle between the thumb and index finger is greater than 90 degrees (indicating a stretched hand, suitable for a right-click gesture)\n",
    "        get_angle(landmark_list[5], landmark_list[6], landmark_list[8]) > 90 and\n",
    "        \n",
    "        # Check if the distance between the thumb and index finger is greater than a threshold (indicating that the fingers are apart, suitable for the gesture)\n",
    "        thumb_index_dist > 50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8e7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect a double-click gesture based on hand landmarks and thumb-index finger distance\n",
    "def is_double_click(landmark_list, thumb_index_dist):\n",
    "    return (\n",
    "        # Check if the angle between the thumb and index finger is less than 50 degrees (indicating the fingers are close, forming a \"pinch\")\n",
    "        get_angle(landmark_list[5], landmark_list[6], landmark_list[8]) < 50 and\n",
    "        \n",
    "        # Check if the angle between the middle finger and index finger is less than 50 degrees (indicating the fingers are close, forming another \"pinch\")\n",
    "        get_angle(landmark_list[9], landmark_list[10], landmark_list[12]) < 50 and\n",
    "        \n",
    "        # Check if the distance between the thumb and index finger is greater than a threshold (indicating the fingers are apart enough for a precise gesture)\n",
    "        thumb_index_dist > 50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "660cd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect a screenshot gesture based on hand landmarks and thumb-index finger distance\n",
    "def is_screenshot(landmark_list, thumb_index_dist):\n",
    "    return (\n",
    "        # Check if the angle between the thumb and index finger is less than 50 degrees (indicating the fingers are close, forming a \"pinch\")\n",
    "        get_angle(landmark_list[5], landmark_list[6], landmark_list[8]) < 50 and\n",
    "        \n",
    "        # Check if the angle between the middle finger and index finger is less than 50 degrees (indicating the fingers are close, forming another \"pinch\")\n",
    "        get_angle(landmark_list[9], landmark_list[10], landmark_list[12]) < 50 and\n",
    "        \n",
    "        # Check if the distance between the thumb and index finger is less than a threshold (indicating the fingers are very close, possibly for a screenshot gesture)\n",
    "        thumb_index_dist < 50\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a3313e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect hand gestures and perform corresponding actions (e.g., mouse movements, clicks, screenshot)\n",
    "def detect_gesture(frame, landmark_list, processed):\n",
    "    # Ensure there are enough landmarks (at least 21) for gesture detection\n",
    "    if len(landmark_list) >= 21:\n",
    "\n",
    "        # Get the index finger tip position and the distance between thumb and index fingers\n",
    "        index_finger_tip = find_finger_tip(processed)\n",
    "        thumb_index_dist = get_distance([landmark_list[4], landmark_list[5]])\n",
    "\n",
    "        # If the distance between thumb and index finger is small and the angle between them is large (indicating a gesture to move the mouse)\n",
    "        if get_distance([landmark_list[4], landmark_list[5]]) < 50 and get_angle(landmark_list[5], landmark_list[6], landmark_list[8]) > 90:\n",
    "            move_mouse(index_finger_tip)  # Move the mouse cursor based on the index finger's tip position\n",
    "        \n",
    "        # If the gesture is detected as a left-click\n",
    "        elif is_left_click(landmark_list, thumb_index_dist):\n",
    "            mouse.press(Button.left)  # Press the left mouse button\n",
    "            mouse.release(Button.left)  # Release the left mouse button\n",
    "            cv2.putText(frame, \"Left Click\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)  # Display \"Left Click\" text on the screen\n",
    "        \n",
    "        # If the gesture is detected as a right-click\n",
    "        elif is_right_click(landmark_list, thumb_index_dist):\n",
    "            mouse.press(Button.right)  # Press the right mouse button\n",
    "            mouse.release(Button.right)  # Release the right mouse button\n",
    "            cv2.putText(frame, \"Right Click\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)  # Display \"Right Click\" text on the screen\n",
    "        \n",
    "        # If the gesture is detected as a double-click\n",
    "        elif is_double_click(landmark_list, thumb_index_dist):\n",
    "            pyautogui.doubleClick()  # Perform a double-click using pyautogui\n",
    "            cv2.putText(frame, \"Double Click\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)  # Display \"Double Click\" text on the screen\n",
    "        \n",
    "        # If the gesture is detected as a screenshot\n",
    "        elif is_screenshot(landmark_list, thumb_index_dist):\n",
    "            im1 = pyautogui.screenshot()  # Take a screenshot using pyautogui\n",
    "            label = random.randint(1, 1000)  # Generate a random label for the screenshot file name\n",
    "            im1.save(f'my_screenshot_{label}.png')  # Save the screenshot with the label as part of the filename\n",
    "            cv2.putText(frame, \"Screenshot Taken\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)  # Display \"Screenshot Taken\" text on the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "389cd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to capture video from the webcam and detect hand gestures in real-time\n",
    "def main():\n",
    "    # Drawing utilities from MediaPipe to draw landmarks on the frame\n",
    "    draw = mp.solutions.drawing_utils\n",
    "    \n",
    "    # Open the webcam for capturing video\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    try:\n",
    "        # Start reading frames from the webcam\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # If no frame is returned, exit the loop\n",
    "\n",
    "            # Flip the frame horizontally to create a mirror effect (user's hand appears like in a mirror)\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Convert the frame from BGR to RGB color space (required by MediaPipe)\n",
    "            frameRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Process the frame through the MediaPipe Hands model\n",
    "            processed = hands.process(frameRGB)\n",
    "\n",
    "            # List to store the landmarks of the hand\n",
    "            landmark_list = []\n",
    "\n",
    "            # If hand landmarks are detected, process them\n",
    "            if processed.multi_hand_landmarks:\n",
    "                hand_landmarks = processed.multi_hand_landmarks[0]  # Assuming only one hand is detected\n",
    "\n",
    "                # Draw the landmarks and connections on the frame\n",
    "                draw.draw_landmarks(frame, hand_landmarks, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Collect the landmark positions (x, y coordinates)\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    landmark_list.append((lm.x, lm.y))\n",
    "\n",
    "            # Detect the gesture based on the landmarks and update the frame\n",
    "            detect_gesture(frame, landmark_list, processed)\n",
    "\n",
    "            # Display the frame with hand landmarks and gesture annotations\n",
    "            cv2.imshow('Frame', frame)\n",
    "\n",
    "            # Break the loop when the user presses the 'q' key\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    finally:\n",
    "        # Release the webcam capture and close any OpenCV windows\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faadc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yourenvname",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
